
# AI Bias Audit Report

This project investigates bias in a machine learning dataset using fairness toolkits and visual analytics. Our goal is to find and reduce unfair treatment of sensitive groups such as gender, age, or race—ensuring that AI systems are ethical and fair.

##  What We Did

- Chose a real-world dataset with sensitive attributes
- Used fairness metrics to detect potential bias
- Applied bias mitigation techniques to improve fairness
- Visualized and explained the impact of these techniques
- Documented everything in a Jupyter Notebook and presented it in a short talk

##  Tools & Technologies

- Python
- Jupyter Notebook
- IBM AI Fairness 360
- Pandas, NumPy, Matplotlib, Seaborn
- Scikit-learn

## Fairness Metrics Used

- Statistical Parity
- Disparate Impact
- Equal Opportunity

##  Bias Mitigation Techniques

- Reweighing
- Disparate Impact Remover
- Optimized Preprocessing

##  Visualization

We included visual charts that show:
- Where bias existed in the original dataset
- How fairness improved after applying mitigation techniques

##  Why This Matters

AI is being used in important areas like hiring, banking, and healthcare. If it’s biased, it can cause real harm. This project helps us understand and reduce these risks, promoting responsible AI development.

